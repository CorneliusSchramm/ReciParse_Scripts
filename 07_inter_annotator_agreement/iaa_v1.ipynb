{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f score für entities\n",
    "\n",
    "tp = 0.0 \n",
    "fp = 0.0 \n",
    "fn = 0.0\n",
    "\n",
    "for eg in test_examples: \n",
    "    doc = nlp(eg[\"text\"]) \n",
    "    guesses = set((ent.start_char, ent.eng_char, ent.label_) for ent in doc.ents) \n",
    "    truths = set((span[\"start\"], span[\"end\"], span[\"label\"]) for span in eg[\"spans\"]) \n",
    "    tp += len(guesses.intersection(truths)) \n",
    "    fn += len(truths - guesses) \n",
    "    fp += len(guesses - truths) \n",
    "    precision = tp / (tp+fp+1e-10) \n",
    "    recall = tp / (tp+fn+1e-10) \n",
    "    fscore = (2 * precision * recall) / (precision + recall + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prodigy.models.ner.EntityRecognizer.evaluate()\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "model = EntityRecognizer(nlp, label=['PERSON', 'ORG']) \n",
    "stats = model.evaluate(examples, no_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f score auch für Relations -> sinnvoller miteinander zu vergleichen, als alle Möglichen "
   ]
  }
 ]
}