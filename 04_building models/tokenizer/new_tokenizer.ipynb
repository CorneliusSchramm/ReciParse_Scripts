{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('nightly-env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "cf02395a8bc3c4185a8c7771341cb9d2ba4fc0d0faa3ca76b9610983d773f14e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Writing a stricter tokenizer for the blank German spaCy Model\n",
    "\n",
    "## Tokenizer Theory Notes\n",
    "\n",
    "Tokenizer exception: Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied.\n",
    "\n",
    "Prefix: Character(s) at the beginning, e.g. $, (, “, ¿.\n",
    "\n",
    "Suffix: Character(s) at the end, e.g. km, ), ”, !.\n",
    "\n",
    "Infix: Character(s) in between, e.g. -, --, /, …."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\CocoL\\OneDrive - Universitaet St.Gallen\\2-Academics\\Bachelor\\7-Semester\\Capstone\\ReciParse_Scripts\\04_building models\\tokenizer\n"
     ]
    }
   ],
   "source": [
    "# === SETUP ===\n",
    "\n",
    "# ---Importing---\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# ---Setting Path---\n",
    "print(os.getcwd())\n",
    "\n",
    "# Coco Path\n",
    "path = Path(\"C:/Users/CocoL/Universität St.Gallen/STUD-Capstoneproject Tell 6 - General/02-Coding\")\n",
    "\n",
    "# Jona Path\n",
    "# path = r\"/Users/jhoff/Universität St.Gallen/STUD-Capstoneproject Tell 6 - Dokumente/General/02-Coding\"\n",
    "\n",
    "# Giovanni Path\n",
    "# path = r\"/Users/jonathanebner/Universität St.Gallen/STUD-Capstoneproject Tell 6 - General/02-Coding\"\n",
    "\n",
    "# Leo Path\n",
    "# path = r\"/Users/Leonidas/Universität St.Gallen/STUD-Capstoneproject Tell 6 - General/02-Coding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VARIANTE 1 ===\n",
    "\n",
    "# ---Importing---\n",
    "from spacy.lang.de import German\n",
    "nlp = German()\n",
    "\n",
    "\n",
    "# ---Before---\n",
    "print(\"VORHER:\")\n",
    "print([tok for tok in nlp(\"bedeckt sein).Im ca. 180° vorgeheizten Ofen bei\")]) # \"sein).Im\" -->is bad\n",
    "print([tok for tok in nlp(\"1.Obst waschen(sauber).Und dann \")])\n",
    "print([tok for tok in nlp(\"1.)Herdäpfel sind geil.2)Deine Mum auch.\")])\n",
    "\n",
    "# ---Change Tokenizer---\n",
    "import re\n",
    "def custom_tokenizer(nlp):\n",
    "    \n",
    "    my_prefixes = [r'[0-9]\\.', r\"\\.\", r\"^\\.\"]\n",
    "    \n",
    "    all_prefixes_re = spacy.util.compile_prefix_regex(tuple(list(nlp.Defaults.prefixes) + my_prefixes))\n",
    "    \n",
    "    # Handle ( that doesn't have proper spacing around it\n",
    "    custom_infixes = [r'\\.\\.\\.+', r'(?<=[0-9])-(?=[0-9])', r'[!&:,()]', r\"\\.\"]\n",
    "    infix_re = spacy.util.compile_infix_regex(tuple(list(nlp.Defaults.infixes) + custom_infixes))\n",
    "    \n",
    "    suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)   \n",
    "    \n",
    "    return Tokenizer(nlp.vocab, nlp.Defaults.tokenizer_exceptions,\n",
    "                     prefix_search = all_prefixes_re.search, \n",
    "                     infix_finditer = infix_re.finditer, \n",
    "                     suffix_search = suffix_re.search,\n",
    "                     token_match=None)\n",
    "\n",
    "# Add new Tokenizer to NLP Object\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "# ---After---\n",
    "print(\"\\n\\nNACHER:\")\n",
    "print([tok for tok in nlp(\"bedeckt sein).Im ca. 180° vorgeheizten Ofen bei\")]) \n",
    "print([tok for tok in nlp(\"1.Obst waschen(sauber).Und dann \")])\n",
    "print([tok for tok in nlp(\"1.)Herdäpfel sind geil.2)Deine Mum auch.\")])\n",
    "\n",
    "# ---Saving---\n",
    "nlp.to_disk(path / \"04-Models\" / \"Custom Tokenizer\" / \"de_custom_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VARIANTE 2 ===\n",
    "\n",
    "# ---Importing---\n",
    "from spacy.lang.de import German\n",
    "nlp = German()\n",
    "\n",
    "\n",
    "# ---Before---\n",
    "print(\"VORHER:\")\n",
    "print([tok for tok in nlp(\"bedeckt sein).Im ca. 180° vorgeheizten Ofen bei\")]) # \"sein).Im\" -->is bad\n",
    "print([tok for tok in nlp(\"1.Obst waschen(sauber).Und dann \")])\n",
    "print([tok for tok in nlp(\"1.)Herdäpfel) sind geil.2)Deine Mum auch.\")])\n",
    "\n",
    "\n",
    "# ---Change Tokenizer---\n",
    "import re\n",
    "def custom_tokenizer(nlp):\n",
    "    \n",
    "    my_prefixes = []\n",
    "    \n",
    "    all_prefixes_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "    \n",
    "    # Handle (). that doesn't have proper spacing around it\n",
    "    custom_infixes = [r'[!&:,\\(\\)\\.]']\n",
    "    infix_re = spacy.util.compile_infix_regex(tuple(list(nlp.Defaults.infixes) + custom_infixes))\n",
    "    \n",
    "    suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)   \n",
    "    \n",
    "    return Tokenizer(nlp.vocab, nlp.Defaults.tokenizer_exceptions,\n",
    "                     prefix_search = all_prefixes_re.search, \n",
    "                     infix_finditer = infix_re.finditer, \n",
    "                     suffix_search = suffix_re.search,\n",
    "                     token_match=None)\n",
    "\n",
    "# Add new Tokenizer to NLP Object\n",
    "nlp.tokenizer = custom_tokenizer(nlp) \n",
    "\n",
    "\n",
    "# ---After---\n",
    "print(\"\\n\\nNACHER:\")\n",
    "print([tok for tok in nlp(\"bedeckt sein).Im ca. 180° vorgeheizten Ofen bei\")]) \n",
    "print([tok for tok in nlp(\"1.Obst waschen(sauber).Und dann \")])\n",
    "print([tok for tok in nlp(\"1.)Herdäpfel) sind geil.2)Deine Mum auch.\")])\n",
    "\n",
    "# ---Saving---\n",
    "nlp.to_disk(path / \"04-Models\" / \"Custom Tokenizer\" / \"de_custom_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RANDOM TESTING CELL ===\n",
    "\n",
    "# German.Defaults.prefixes = German.Defaults.prefixes + tuple([r'[0-9]\\.'])\n",
    "\n",
    "# German.Defaults.infixes = German.Defaults.infixes + tuple(['\\.\\.\\.+'])\n",
    "# German.Defaults.infixes = German.Defaults.infixes + tuple(['(?<=[0-9])-(?=[0-9])'])\n",
    "# German.Defaults.infixes = German.Defaults.infixes + tuple(['[!&:,()]'])\n",
    "# nlp.Defaults.prefixes == German.Defaults.prefixes "
   ]
  }
 ]
}