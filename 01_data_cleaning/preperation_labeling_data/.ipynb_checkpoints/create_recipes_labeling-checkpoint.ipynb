{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create four Dataset for labeling the recipes\n",
    "####  14. November 2020\n",
    "### Input\n",
    "Chefkoch Instructions data csv-file\n",
    "### Output\n",
    "8 JSONs (2 per person) <br>\n",
    "batch1: 10 overlap, 50 unique <br>\n",
    "batch2: 20, overlap, 70 unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coco Path\n",
    "# path = r\"C:\\Users\\CocoL\\Universit채t St.Gallen\\STUD-Capstoneproject Tell 6 - General\\\"\n",
    "# Jona Path\n",
    "# path = r\"/Users/jhoff/Universit채t St.Gallen/STUD-Capstoneproject Tell 6 - Dokumente/General/\"\n",
    "# Giovanni Path\n",
    "path = r\"/Users/jonathanebner/Universit채t St.Gallen/STUD-Capstoneproject Tell 6 - General\"\n",
    "# Leo Path\n",
    "#ath = r\"/Users/Leonidas/Universit채t St.Gallen/STUD-Capstoneproject Tell 6 - General/\"\n",
    "\n",
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of imported recipes:  12190\n"
     ]
    }
   ],
   "source": [
    "# Load data and seperate instructions at @ as defined\n",
    "data = pd.read_csv(path+\"/02-Coding/01-Data/01_instructions/instructions.csv\", sep=\"@\", encoding=\"utf-16\", header=None, error_bad_lines=False)\n",
    "print(\"Number of imported recipes: \",len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name column text to have the keys named text later\n",
    "data.columns = [\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'de_core_news_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-357cbdda4f33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initiate nlp model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdisabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'parser'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tagger'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ner'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'de_core_news_lg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create a column with tokenized instructions using spacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'de_core_news_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# Initiate nlp model\n",
    "disabled = ['parser', 'tagger', 'ner']\n",
    "nlp = spacy.load('de_core_news_lg', disable=disabled)\n",
    "\n",
    "# Create a column with tokenized instructions using spacy\n",
    "data[\"Tokenized\"] = data[\"text\"].apply(nlp) \n",
    "# Create a column with the amount of tokens per recipe\n",
    "data[\"Length\"] = data[\"Tokenized\"].apply(len)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all recipes with a token length of smaller 210\n",
    "data_smaller210 = data[data[\"Length\"] < 210]\n",
    "data_smaller210.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete duplicates in text\n",
    "data_smaller210_nodupl = data_smaller210.drop_duplicates(subset='text', keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All:\", len(data))\n",
    "print(\"Length 210:\", len(data_smaller210))\n",
    "print(\"No duplicates:\", len(data_smaller210_nodupl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle all recipes\n",
    "data_shuffle = data_smaller210_nodupl.sample(frac=1).reset_index(drop=True).iloc[:,0]\n",
    "data_final = data_shuffle.to_frame()\n",
    "data_drop = data_final.copy()\n",
    "data_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create overlap of 30\n",
    "overlap = data_drop.sample(30)\n",
    "overlap.to_json(path+\"/02-Coding/01-Data/20_overlap/overlap.json\",orient='records',force_ascii=False)\n",
    "overlap_batch1 = overlap.iloc[:10]\n",
    "overlap_batch2 = overlap[10:]\n",
    "data_drop = data_drop.drop(list(overlap.index))\n",
    "print(\"Overlap:\", len(overlap))\n",
    "print(\"Rest:\", len(data_drop))\n",
    "overlap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_recipes_batch1 = 50\n",
    "amount_recipes_batch2 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch1 and batch2\n",
    "\n",
    "batch1_jona = data_drop.sample(amount_recipes_batch1)\n",
    "data_drop = data_drop.drop(list(batch1_jona.index))\n",
    "batch2_jona = data_drop.sample(amount_recipes_batch2)\n",
    "data_drop = data_drop.drop(list(batch2_jona.index))\n",
    "print(\"Rest:\", len(data_drop))\n",
    "\n",
    "batch1_leo = data_drop.sample(amount_recipes_batch1)\n",
    "data_drop = data_drop.drop(list(batch1_leo.index))\n",
    "batch2_leo = data_drop.sample(amount_recipes_batch2)\n",
    "data_drop = data_drop.drop(list(batch2_leo.index))\n",
    "print(\"Rest:\", len(data_drop))\n",
    "\n",
    "batch1_coco = data_drop.sample(amount_recipes_batch1)\n",
    "data_drop = data_drop.drop(list(batch1_coco.index))\n",
    "batch2_coco = data_drop.sample(amount_recipes_batch2)\n",
    "data_drop = data_drop.drop(list(batch2_coco.index))\n",
    "print(\"Rest:\", len(data_drop))\n",
    "\n",
    "batch1_jonathan = data_drop.sample(amount_recipes_batch1)\n",
    "data_drop = data_drop.drop(list(batch1_jonathan.index))\n",
    "batch2_jonathan = data_drop.sample(amount_recipes_batch2)\n",
    "data_drop = data_drop.drop(list(batch2_jonathan.index))\n",
    "print(\"Rest:\", len(data_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix batches with overlap\n",
    "\n",
    "batch1_jona_total = pd.concat([batch1_jona, overlap_batch1]).sample(frac = 1)\n",
    "batch2_jona_total = pd.concat([batch2_jona, overlap_batch2]).sample(frac = 1)\n",
    "\n",
    "batch1_leo_total = pd.concat([batch1_leo, overlap_batch1]).sample(frac = 1)\n",
    "batch2_leo_total = pd.concat([batch2_leo, overlap_batch2]).sample(frac = 1)\n",
    "\n",
    "batch1_coco_total = pd.concat([batch1_coco, overlap_batch1]).sample(frac = 1)\n",
    "batch2_coco_total = pd.concat([batch2_coco, overlap_batch2]).sample(frac = 1)\n",
    "\n",
    "batch1_jonathan_total = pd.concat([batch1_jonathan, overlap_batch1]).sample(frac = 1)\n",
    "batch2_jonathan_total = pd.concat([batch2_jonathan, overlap_batch2]).sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df to json\n",
    "def df_to_json_batch_1():\n",
    "    batch1_jona_total.to_json(path+\"02-Coding/01-Data/11_clean_batches/batch1_jona.json\",orient='records',force_ascii=False)\n",
    "\n",
    "    batch1_leo_total.to_json(path+\"02-Coding/01-Data/11_clean_batches/batch1_leo.json\",orient='records',force_ascii=False)\n",
    "\n",
    "    batch1_coco_total.to_json(path+\"02-Coding/01-Data/11_clean_batches/batch1_coco.json\",orient='records',force_ascii=False)\n",
    "\n",
    "    batch1_jonathan_total.to_json(path+\"02-Coding/01-Data/11_clean_batches/batch1_jonathan.json\",orient='records',force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_to_json_batch1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_json_batch_2():\n",
    "    batch2_jona_total.to_json(path+\"02-Coding/01-Data/11_clean_batches/batch2_jona.json\",orient='records',force_ascii=False)\n",
    "\n",
    "    batch2_leo_total.to_json(path+\"02-Coding/01-Data/11_clean_batches/batch2_leo.json\",orient='records',force_ascii=False)\n",
    "\n",
    "    batch2_coco_total.to_json(path+\"02-Coding/01-Data/11_clean_batches/batch2_coco.json\",orient='records',force_ascii=False)\n",
    "\n",
    "    batch2_jonathan_total.to_json(path+\"02-Coding/01-Data/11_clean_batches/batch2_jonathan.json\",orient='records',force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_json_batch2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
